{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'sys' (built-in)>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import jieba\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "import sys\n",
    "import imp\n",
    "imp.reload(sys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载文件，导入数据,分词\n",
    "def loadfile():\n",
    "    neg = pd.read_excel('data/neg.xls',header=None,index=None)\n",
    "    pos = pd.read_excel('data/pos.xls',header=None,index=None)\n",
    "    # neg[0] 和 pos[0] 数据（成列，一列一条文本）\n",
    "    \n",
    "    \n",
    "    cw = lambda x: list(jieba.cut(x))\n",
    "    pos['words'] = pos[0].apply(cw)\n",
    "    neg['words'] = neg[0].apply(cw)\n",
    "    # neg['words‘] 和 pos['words'] 存放分词后的结果，一条文本处理后是一个list\n",
    "    \n",
    "    # use 1 for positive sentiment, 0 for negative，得到所有两类样本的labels\n",
    "    y = np.concatenate((np.ones(len(pos)), np.zeros(len(neg))))\n",
    "\n",
    "    # 按给定比例随机划分训练集和测试集\n",
    "    x_train, x_test, y_train, y_test = train_test_split(np.concatenate((pos['words'], neg['words'])), y, test_size=0.2)\n",
    "    \n",
    "    np.save('svm_data/y_train.npy',y_train)\n",
    "    np.save('svm_data/y_test.npy',y_test)\n",
    "    return x_train,x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train,x_test=loadfile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对每个句子的所有词向量取均值作为每个评论的输入: sum each word's (1, n_dim) ,then divide it by num of words in a text\n",
    "# 这样做忽略了单词之间的排序顺序对情感分析的影响\n",
    "def buildWordVector(text, size, imdb_w2v):\n",
    "    vec = np.zeros(size).reshape((1, size))\n",
    "    count = 0.\n",
    "    for word in text:\n",
    "        try:\n",
    "            vec += imdb_w2v[word].reshape((1, size))    # (300,) -> (1, 300)\n",
    "            count += 1.\n",
    "        except KeyError:\n",
    "            continue\n",
    "    if count != 0:\n",
    "        vec /= count\n",
    "    return vec\n",
    "\n",
    "# 训练词向量，分别对x_train 和 x_test 得到各自 (num_examples, n_dim) 的映射矩阵\n",
    "def get_train_vecs(x_train,x_test):\n",
    "    n_dim = 300\n",
    "    # Initialize model and build vocab\n",
    "    imdb_w2v = Word2Vec(size=n_dim, min_count=10)\n",
    "    imdb_w2v.build_vocab(x_train)\n",
    "    \n",
    "    # Train the model over train_reviews (this may take several minutes)\n",
    "    imdb_w2v.train(sentences=x_train, total_examples=imdb_w2v.corpus_count, epochs=imdb_w2v.epochs)\n",
    "    \n",
    "    train_vecs = np.concatenate([buildWordVector(z, n_dim,imdb_w2v) for z in x_train])\n",
    "    # train_vecs = scale(train_vecs)\n",
    "    \n",
    "    np.save('svm_data/train_vecs.npy',train_vecs)\n",
    "    print(train_vecs.shape)\n",
    "    \n",
    "    # Train word2vec on test tweets\n",
    "    imdb_w2v.train(sentences=x_test, total_examples=imdb_w2v.corpus_count, epochs=imdb_w2v.epochs)\n",
    "    imdb_w2v.save('svm_data/w2v_model/w2v_model.pkl')\n",
    "     \n",
    "    # Build test tweet vectors then scale\n",
    "    test_vecs = np.concatenate([buildWordVector(z, n_dim,imdb_w2v) for z in x_test])\n",
    "    \n",
    "    # test_vecs = scale(test_vecs)\n",
    "    np.save('svm_data/test_vecs.npy',test_vecs)\n",
    "    \n",
    "    print(test_vecs.shape)\n",
    "    print('data preprocessing and word embedding finished...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "至此完成所有预处理和embedding，并将相应的参数矩阵保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    train_vecs = np.load('svm_data/train_vecs.npy')\n",
    "    y_train = np.load('svm_data/y_train.npy')\n",
    "    test_vecs = np.load('svm_data/test_vecs.npy')\n",
    "    y_test = np.load('svm_data/y_test.npy') \n",
    "    return train_vecs,y_train,test_vecs,y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # explore the data\n",
    "# train_vecs,y_train,test_vecs,y_test = get_data()\n",
    "# print(x_train.shape)\n",
    "# print(x_test.shape)\n",
    "# print(train_vecs.shape)\n",
    "# print(test_vecs.shape)\n",
    "# print(y_train.shape)\n",
    "# print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "##训练svm模型\n",
    "def svm_train(train_vecs,y_train,test_vecs,y_test):\n",
    "    clf = SVC(kernel='rbf',verbose=True)\n",
    "    clf.fit(train_vecs,y_train)\n",
    "    joblib.dump(clf, 'svm_data/svm_model/model.pkl')\n",
    "    print(clf.score(test_vecs,y_test))\n",
    "    \n",
    "    \n",
    "##得到待预测单个句子的词向量    \n",
    "def get_predict_vecs(words):\n",
    "    n_dim = 300\n",
    "    imdb_w2v = Word2Vec.load('svm_data/w2v_model/w2v_model.pkl')\n",
    "    train_vecs = buildWordVector(words, n_dim,imdb_w2v)\n",
    "\n",
    "    return train_vecs\n",
    "    \n",
    "####对单个句子进行情感判断    \n",
    "def svm_predict(string):\n",
    "    words = jieba.lcut(string)   # 先把要分析的句子进行分词\n",
    "    words_vecs = get_predict_vecs(words)  # 对分词后的结果求词向量\n",
    "    clf = joblib.load('svm_data/svm_model/model.pkl')\n",
    "     \n",
    "    result = clf.predict(words_vecs)\n",
    "    \n",
    "    if int(result[0]) == 1:\n",
    "        print(string,' positive')\n",
    "    else:\n",
    "        print(string,' negative')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/5y/czdghcd91q3gnr9rmh8ft7040000gn/T/jieba.cache\n",
      "Loading model cost 0.886 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "/Users/chaojunwang/anaconda2/envs/python36/lib/python3.6/site-packages/ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16884, 300)\n",
      "(4221, 300)\n",
      "data preprocessing and word embedding finished...\n",
      "[LibSVM]0.8012319355602938\n",
      "电池充完了电连手机都打不开.简直烂的要命.真是金玉其外,败絮其中!连5号电池都不如  negative\n"
     ]
    }
   ],
   "source": [
    "def train():\n",
    "    x_train,x_test=loadfile() #得到句子分词后的结果，并把类别标签保存为y_train。npy,y_test.npy\n",
    "    get_train_vecs(x_train,x_test) #计算词向量并保存为train_vecs.npy,test_vecs.npy\n",
    "    train_vecs,y_train,test_vecs,y_test=get_data()#导入训练数据和测试数据\n",
    "    svm_train(train_vecs,y_train,test_vecs,y_test)#训练svm并保存模型\n",
    "    \n",
    "\n",
    "if __name__=='__main__':\n",
    "    \n",
    "    train()\n",
    "    \n",
    "    # 对输入句子情感进行判断\n",
    "    string='电池充完了电连手机都打不开.简直烂的要命.真是金玉其外,败絮其中!连5号电池都不如'\n",
    "    #string='牛逼的手机，从3米高的地方摔下去都没坏，质量非常好'    \n",
    "    svm_predict(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
